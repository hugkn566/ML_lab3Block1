---
title: "Lab3"
author: "Hugo Knape & Zahra Jalil Pour & Niklas Larsson"
date: "12/3/2020"
output: pdf_document
---

# State of contribution 
### Assignment1: Hugo Knape
### Assignment2: Zahra Jalilpour
### Assignment3: Niklas Larsson
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE, include = FALSE}
library(ggplot2)
library(geosphere)
library(dplyr)

```

# Assignment 1. Kernel Methods

```{r}
set.seed(1234567890)
stations <- read.csv("stations.csv")
temps <- read.csv("temps50k.csv")
st <- merge(stations,temps,by="station_number")


a <- 58.4274 # The point to predict (up to the students)
b <- 14.826
p2 <- c(a,b)
date_pred <- "2003-07-04" # The date to predict (up to the students)
times <- c("04:00:00", "06:00:00", "08:00:00","10:00:00","12:00:00","14:00:00","16:00:00" ,
           "18:00:00","20:00:00","22:00:00","24:00:00")
temp <- vector(length=length(times))
st <- st %>% filter(as.Date(date)<as.Date(date_pred))


#### DISTANCE

dist <- st[,c(1,2,4,5)]
stations <- unique(dist)
physical_dist <- distHaversine(p2, stations[,3:4])
stations$dist <- physical_dist
st_2  <- merge(stations,st,by="station_number")
distance <- st_2$dist


####DAYS

library(lubridate)
date_df <- as.Date(st$date)
year(date_df) <- 2020
date_pred <- as.Date(date_pred)
year(date_pred) <- 2020
diff_days <- as.numeric(abs(date_pred-date_df))
diff_days <- ifelse(diff_days<183, diff_days, 365-diff_days)
st$diff_days <- diff_days
### HOUR

times <- strptime(times, format = "%H:%M:%S")
st$time <- strptime(st$time, format = "%H:%M:%S")
time_diff <- data.frame(diff_4 =rep(0,nrow(st)), diff_6 = rep(0,nrow(st)), diff_8 =rep(0,nrow(st))
                        , diff_10 = rep(0,nrow(st)), diff_12 = rep(0,nrow(st)), diff_14 = rep(0,nrow(st))
                        , diff_16 = rep(0,nrow(st)), diff_18 = rep(0,nrow(st)), diff_20 =rep(0,nrow(st))
                        , diff_22 =rep(0,nrow(st)), diff_24 = rep(0,nrow(st)))
for (i in 1:ncol(time_diff)) {
  time_diff[,i] <- (as.numeric(abs(difftime(times[i], st$time, unit = "hours"))))
  time_diff[,i] <- ifelse(time_diff[,i]<12, time_diff[,i] , 24-time_diff[,i])
}
```

First we do some data processing. We compute the distance between the by using distHaversine. For the days we begin to set all dates to the same year to be able to count the exact days diffs it is. After that we use ifelse to be able to get the correct number of days between to dates. For the hour we compute the the difference between times for all the different timepoints. We use ifelse here as well.


#### Kernels

```{r}
par(mfrow=c(1,3))
#Distance smoothing
h_distance <- 300000
kernel_distance <- exp(-(distance/h_distance)^2)
plot(distance, kernel_distance, main = "Distance kernel")

#Day smoothing 
h_date <- 30
kernel_date <- exp(-(diff_days/h_date)^2)
plot(diff_days, kernel_date, main = "Date kernel")

#Hour smoothing
h_time <- 3
kernel_hour <- exp(-(time_diff/h_time)^2)
plot(time_diff[,1], kernel_hour$diff_4, main = "Hour kernel, hour 4")
```

Here we can see the 3 different kernels for Distance, Date and Hour. For distance we choosed h 30000, for date h 30 and for hour h 3. This is very subjectively chosen 
but it feels reasonable when we have weather data.

#### Predicition, Multiplying Kernels. 

```{r}
#### MULTIPLYING KERNELS ####

for (i in 1:ncol(kernel_hour)){
 temp[i] <-  sum(kernel_distance*kernel_date*kernel_hour[,i]*st$air_temperature)/
   sum(kernel_distance*kernel_date*kernel_hour[,i])
}
mult_kernels <- data.frame(temp, times)
plot(mult_kernels$times, mult_kernels$temp, type = "o", main = "Prediction for 2003-07-04, multiplying", ylab = "Temp", xlab = "Hour")


```
The plot above shows the predicted temperature for 4th July 2003 for different hours with multiplied kernels. We can see that the temperature looks reasonable for the model with lower temperatures and morning and night and high temperature around 12.00.

#### Predicition, Addition kernels.

```{r}
#### ADDING KERNELS ####

for (i in 1:ncol(kernel_hour)){
  temp[i] <-  sum((kernel_distance+kernel_date+kernel_hour[,i])*st$air_temperature)/
    sum(kernel_distance+kernel_date+kernel_hour[,i])
}
add_kernels <- data.frame(temp, times)
plot(add_kernels$times, add_kernels$temp, type = "o", main = "Prediction for 2003-07-04, addition",
      ylab = "Temp", xlab = "Hour")
```

The plot above shows the predicted temperature for 4th July 2003 for different hours with summing kernels. We can see that the temperature looks not reasonable for the model with very low temperatures for the whole day and that it only differs one degree on the whole day. 

#### Conclusion 

We can see that the model with multiplying kernels are much better than the model with summing kernels. This may be due to when we add kernels together, a kernel with very small values get no impact at all and kernels with large values gets all the impact. If we compare that with when we multiplies kernels all kernels are still given an effect to the prediction because even if the value is small or big. 


# Assignment 2.


# Assigment 3. Neural Networks

```{r, echo=FALSE}
library(neuralnet)
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
```

```{r}
# Random initialization of the weights in the interval [-1, 1]
set.seed(12345)
nlay = c(16,8,4) # Number of layers and neurons for each layer
winit = runif(10000,-1,1) #Exaggerated to ensure it covers the number of weights (else function will randomize its own)
nn <- neuralnet(Sin ~ Var, tr, hidden = nlay , startweights = winit,  threshold = 1e-4, lifesign = "minimal")
#plot(nn)

```
The result of this neural net is very good as seen in the graph below, it follows almost perfectly.
Three hidden layers was used with 16, 8 and 4 neurons at each respective layer. The convergence threshold was change to 1e-04.
```{r, echo=FALSE}
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2, main = "Training vs Testing vs Predicted")
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1)
legend("bottomright", legend = c("Training", "Testing", "Predicted"), col = c("black","blue","red"), pch = c("o","o","o"))
```

Using the same model to predict from another uniform distribution in the interval of [0,20] gives mixed results. The model predicts very good in the interval [0,10] which it was trained on but does not predict at all in (10,20]. This means that the model is not generalized and can only predict values from what it has seen before, i.e. it does not capture the periodical behaivour of the sin-function.
```{r, echo=FALSE}
# Generate new data
set.seed(1234567890)
Var <- runif(500, 0, 20)
mydata2 <- data.frame(Var, Sin=sin(Var))

# Plot of the training data (black), test data (blue), and predictions (red)
plot(mydata2[,1], predict(nn,mydata2), col="red", cex=1, main = "Predicting from interval [0,20]", ylab = "Sin", xlab = "Var")
points(mydata2[,1], mydata2[,2], col = "blue", cex=1)
legend("bottomleft", legend = c("Predicted", "True"), col = c("red","blue"), pch = c("o","o"))
```


```{r}

set.seed(1234567890)
Var <- runif(500, 0, 20)
mydata3 <- data.frame(Sin=sin(Var), Var)
tr3 <- mydata[1:25,] # Training
te3 <- mydata[26:500,] # Test

# Random initialization of the weights in the interval [-1, 1]
set.seed(12345)
nlay3 = c(2,2,1) # Number of layers and neurons for each layer
winit3 = runif(100000,-1,1) #Exaggerated to ensure it covers the number of weights (else function will randomize its own)
nn3 <- neuralnet(Var ~ Sin, mydata3, hidden = nlay3, startweights = winit3, lifesign = "full", stepmax = 5e4)

```
The number of of neurons where tuned to make it converge at the default threshold.
Predicting the opposite direction, from $sin(x) \rightarrow x$, gives bad results. The predicted result is approximated to the mean of the true output values (the interval $[0,20]$). The correct mathematical solution would be to use $arcsine(sin(x))$ but as this functions output is only defined in the interval $[-\frac{\pi}{2},\frac{\pi}{2}]$ the model will not be able to predict this.

In other words: The sine function will only output values $[-1,1]$ no matter how large or small the input due to its' periodic nature. When the model is trained on data where the input is between $[-1,1]$ and output ranges $[0,20]$ it does not find the pattern to follow as some information is "lost" in the sine-function. It might be possible to train a model to learn this but it would require more time and most likley a different approach.

In the graph below it is shown that it would not be possible to predict in this direction.
```{r, echo=FALSE}
# Plot of the training data (black), test data (blue), and predictions (red)
plot(mydata3, cex=2, main = "Predicting x from Sin(x)", ylim = c(-8,20), xlab = "Sin(x)",ylab="x" )
pred = predict(nn3,mydata3)
points(mydata3$Sin, pred, col="red", cex=1)
points(mydata3$Sin, asin(mydata3$Sin), col="blue", cex=1)
legend("bottom", legend = c("x","Predicted","Arcsin(Sin(x))"), col = c("black","red","blue"), pch = c("o","o","o"))
```
